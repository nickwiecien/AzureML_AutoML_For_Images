{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a140bd2-0a49-4caf-9e05-8aea5f3916c6",
   "metadata": {},
   "source": [
    "# Azure Machine Learning - AutoML for Images Training Pipeline\n",
    "This notebook demonstrates creation of an Azure ML pipeline designed to load image data from an AML-linked blob storage account, convert that data into a labeled dataset (.jsonl format) using blob tags retrieved via the Azure blob storage SDK, and submit an AutoML for Images run to train a new object detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e095e-e868-4753-90df-767b1cd1c2a7",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e24e8-6a03-4429-a68d-896d870b68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset, Model\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DataFactoryCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE, DEFAULT_GPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064490f3-240a-49f5-9420-c2f4a05fce44",
   "metadata": {},
   "source": [
    "### Connect to AML workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3a583-98b1-44e7-ace6-32f31b1edf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406b935c-1c2a-46cf-a9f4-1a0345332ea5",
   "metadata": {},
   "source": [
    "### Create and connect to ML training cluster\n",
    "Connect to workspace using config associated config file. Get a reference to you pre-existing AML compute cluster or provision a new cluster to facilitate processing. Finally, get references to your default blob datastore. Note: For image classification model training purposes, we leverage GPU compute for our training cluster. The default cluster definition below is configured to spin down individual nodes after 3 minutes of inactivity. This automated spin-down will help decrease training costs. Additionally, we are creating a test cluster for deploying our custom model to a real-time endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e232fa0-e2ea-42b3-8392-0b0aea9f5442",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found existing compute target.\")\n",
    "except Exception:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=os.getenv(\"Standard_NC6\"),\n",
    "        idle_seconds_before_scaledown=180,\n",
    "        min_nodes=0,\n",
    "        max_nodes=5,\n",
    "    )\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=20\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3ffd38-e217-4924-aefe-c65dcdc67c5e",
   "metadata": {},
   "source": [
    "### Create Run Configuration\n",
    "The `RunConfiguration` defines the environment used across all python steps. You can optionally add additional conda or pip packages to be added to your environment. [More details here](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py).\n",
    "\n",
    "Here, we also register the environment to the AML workspace so that it can be used for future retraining and inferencing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc2213-bd5a-46b6-a653-82b7b10bb2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment.docker.base_image = DEFAULT_GPU_IMAGE\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create()\n",
    "run_config.environment.python.conda_dependencies.add_conda_package(\"numpy==1.18.5\")\n",
    "run_config.environment.python.conda_dependencies.add_conda_package(\"libffi=3.3\")\n",
    "run_config.environment.python.conda_dependencies.set_pip_requirements([\n",
    "    \"azureml-core==1.37.0\",\n",
    "    \"azureml-mlflow==1.37.0\",\n",
    "    \"azureml-dataset-runtime==1.37.0\",\n",
    "    \"azureml-telemetry==1.37.0\",\n",
    "    \"azureml-responsibleai==1.37.0\",\n",
    "    \"azureml-automl-core==1.37.0\",\n",
    "    \"azureml-automl-runtime==1.37.0\",\n",
    "    \"azureml-train-automl-client==1.37.0\",\n",
    "    \"azureml-defaults==1.37.0\",\n",
    "    \"azureml-interpret==1.37.0\",\n",
    "    \"azureml-train-automl-runtime==1.37.0\",\n",
    "    \"azureml-automl-dnn-vision==1.37.0\",\n",
    "    \"azureml-dataprep>=2.24.4\"\n",
    "])\n",
    "run_config.environment.python.conda_dependencies.set_python_version('3.7')\n",
    "run_config.environment.name = \"AutoMLForImagesEnv\"\n",
    "run_config.environment.register(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19629ce6-7554-4497-b063-da94b2a11649",
   "metadata": {},
   "source": [
    "### Define Pipeline Parameters\n",
    "`PipelineParameter` objects serve as variable inputs to an Azure ML pipeline and can be specified at runtime. Below we define the following parameters for our Azure ML Pipeline:\n",
    "\n",
    "| Parameter Name | Parameter Description |\n",
    "|----------------|-----------------------|\n",
    "| `model_name` | Name of the custom object detection model to be trained (used for model registration). |\n",
    "| `dataset_name` | The name of the dataset to be created (using images from the attached datastore) upon execution of the pipeline. |\n",
    "| `compute_name` | Name of the compute cluster to be used for model training. | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c8fc8-4e10-4d80-855b-eda6fbdaaa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = PipelineParameter(name='model_name', default_value='Model_Name')\n",
    "dataset_name = PipelineParameter(name='dataset_name', default_value='Dataset_Name')\n",
    "compute_name = PipelineParameter(name='compute_name', default_value=cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ebdcea-a4eb-4686-81bf-7d7a3a32d3c3",
   "metadata": {},
   "source": [
    "### Define Pipeline Steps\n",
    "The pipeline below consists of a single step which executes an associated python script located in the `./pipeline_step_scripts` dir. In this step we call the script located at `./pipeline_step_scripts/automl_job.py` which retrieves an Image Dataset from the AML workspace (referenced by the `dataset_name` parameter and triggers execution of an AutoML for Images training job. Upon completion of this job, the trained model is automatically registered in the AML workspace according to the value provided in the `model_name` parameter.\n",
    "\n",
    "<i>Note:</i> The AutoML configuration settings can be modified inline by editing the `automl_job.py` file. Additionally, certain fields can be added as `PipelineParameters` and passed into the executed python script step. Finally, advanced logic to perform A/B testing against newly trained models and historical best-performers can be integrated into this step (or a secondary step) to ensure the registered model is always the best performer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bda067-d4be-4a6f-b513-dcc780927963",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_job_step = PythonScriptStep(\n",
    "    name='Submit AutoML for Images Training Job',\n",
    "    script_name='automl_job.py',\n",
    "    arguments=[\n",
    "        '--model_name', model_name,\n",
    "        '--dataset_name', dataset_name,\n",
    "        '--compute_name', compute_name,\n",
    "    ],\n",
    "    compute_target=compute_target,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed007568-49f7-4062-b3af-cfb28bd6f515",
   "metadata": {},
   "source": [
    "### Create Pipeline\n",
    "Create an Azure ML Pipeline by specifying the steps to be executed. Note: based on the dataset dependencies between steps, exection occurs logically such that no step will execute unless all of the necessary input datasets have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef57ab6c-3f34-4af6-9532-d5b325bb25f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[submit_job_step] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a54616-c6d7-4b7c-9a48-fb4fd8126537",
   "metadata": {},
   "source": [
    "### Trigger a Pipeline Execution from the Notebook\n",
    "You can create an Experiment (logical collection for runs) and submit a pipeline run directly from this notebook by running the commands below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee306124-19c4-4753-8a04-096d12c8ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, 'MY_EXPERIMENT')\n",
    "run = experiment.submit(pipeline, pipeline_parameters = {\"model_name\": \"MY_MODEL\", \"dataset_name\": \"MY_DATASET\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
